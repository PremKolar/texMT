This section walks through the algorithm step by step, so as to explain
which
methods are used and how they are implemented.
The idea is that the code from step \mcode{S00..} on can only accept one well
defined structure of data. In earlier versions the approach was to write code
that would adapt to different types of data automatically. All of this extra
adaptivity turned out to visually and structurally clog the code more than it
did offer much of a benefit. The concept was therefor reversed.
\mcode{S00_prep_data} can be altered to produce required output. Yet, there
should be no need to adapt any of the later steps in any way.
All input parameters are to be set in \mcode{INPUT.m} and \mcode{INPUT}\textit{xxx}\mcode{.m}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Step S00: Prepare Data}
\mcode{function S00_prep_data}	\\
Before the actual eddy detection and tracking is performed,  SSH-, latitude- and longitude-data is extracted from the given data at desired geo-coordinate bounds and saved as structures in the form needed by the next step (S01). This step also builts the file \mcode{window.mat} via \mcode{GetWindow3} which saves geometric information about the input and output data as well as a cross-referencing index-matrix which is used to reshape all \textit{cuts} to the user defined geo-coordinate-geometry. The code can handle geo-coordinate input that crosses the longitudinal seam of
the input data. \Eg say the input data comes in matrices that start and end on
some (not necessarily strictly meridional) line straight across the Pacific and
it is the Pacific only that is to be analyzed for eddies, the output maps are
stitched accordingly. In the zonally continous case \ie the full-longitude case, an \textit{overlap} in x-direction across the \textit{seam}-meridian of the chosen map is included so that contours across the seam can be detected and tracked across it. One effect is that eddies in proximity to the seam can get detected twice at both zonal ends of the maps. The redundant \textit{ghost}-eddies get filtered out in \mcode{S05_track_eddies}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \section{Step S01: Find Contours}
\mcode{function S01_contours}\\
The sole purpose of this step is to apply MATLAB's \mcode{contourc.m} function
to the SSH data. It simply saves one file per time-step with all contour indices
appended into one vector \footnote{see the MATLAB documentation.}. The contour
intervals are determined by the user defined increment and range from the
minimum- to the maximum of given SSH data. \\
The function \mcode{initialise.m}, which is called at the very beginning of
every step, here has the purpose of rechecking the \textit{cuts} for
consistency and correcting the time-steps accordingly (\ie when files are
missing). \mcode{initialise.m} also distributes the files to the threads \ie
parallelization is in time dimension.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Step S01b: Find Mean Rossby Radii and Phase Speeds}
\mcode{function S01b_BruntVaisRossby}\\

This function...
\begin{itemize}
	\item
	\begin{itemize}
		\item
		...calculates the pressure $P(z,\phi)$ in order to...
		\item
		...calculate the Brunt-V\"ais\"al\"a-Frequency according to $N^{2}(S,T,P,\phi)=-\frac{g(\phi)}{P} \frac{\pr \rho(S,T,P)}{\pr z}$
		in order to...	
	\end{itemize}
	\item
	\begin{itemize}
		\item
		...integrate the Rossby-Radius $\Lr=\frac{1}{\pi f}\INT{H}{ }{N}{z}$ and ...
		\item
		apply the long-Rossby-Wave dispersion relation to found $\Lr$ to estimate Rossby-Wave phase-speeds $c=-\frac{\beta}{k^2+(1/L_r)^2} \approx -\beta \Lr^2$
	\end{itemize}
\end{itemize}

The 3-dimensional matrices ($S$ and $T$) are cut zonally into slices which then get distributed to the threads. This allows for matrix operations for all calculations which would otherwise cause memory problems due to the immense sizes of the 3d-data \footnote{\Eg the pop data has dimensions $42 \times 3600 \times 1800 $.}.   


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Step S02: Calculate Geostrophic Parameters}
\mcode{function S02_infer_fields}\\ 
This step reads the cut SSH data from \mcode{S00_prep_data} to perform 2 steps:
\begin{enumerate}
\item
Calculate a mean over time of $SSH(y,x)$.
	\item 
\begin{itemize}
	\item  use one of the files' geo-information to determine $\f$, $\dfdy$,
$\g$ and the ratio $\g/\f$.
\item
 calculate geostrophic fields from SSH gradients.
 \item
 calculate deformation fields (vorticity, divergence, stretch and shear) via the
fields supplied by the last step.
\item calculate $\okubo$.
\item
Subtract the mean from step 1 from each $SSH(t)$ to filter out persistent SSH-gradients \eg across the Gulf-Stream.
\end{itemize}


\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Step S04: Filter Eddies} \label{S:04}
\mcode{function S04_filter_eddies}\\ 
Since indices of all possible contour lines at chosen levels are available at
this point, it is now time to subject each and every contour to a
myriad of tests to decide whether it qualifies as the outline of an eddy as
defined by the user input threshold parameters.
\subsection*{Reshape for Filtering and Correct out of Bounds Values}
\mcode{function eddies2struct}\\
\mcode{function CleanEddies}\\
In the first step the potential eddies are transformed to a more sensible
format, that is, a structure \mcode{Eddies(EddyCount)} where \mcode{EddyCount}
is the number of all contours. The struct has fields for level, number of
vertices, exact \ie interpolated coordinates and rounded integer coordinates.\\
The interpolation of \mcode{contourc.m} sometimes creates indices that are
either smaller than $0.5$ or larger than $N+0.5$ \footnote{where $N$ is the
domain size} for contours that lie along a boundary. After rounding, this
seldomly leads to indices of either $0$ or $N+1$. These values get set to $1$
and $N$ respectively in this step.
\subsection*{Descent/Ascend Water Column and Apply Checks}
\mcode{function walkThroughContsVertically} \\
The concept of this step is a direct adaption of the algorithm described
by \cite{Chelton2011}. It is split into two steps, one for anti-cyclones and one
for cyclones. Consider \eg the anti-cyclone situation. Since all
geostrophic anti-cyclones are regions of relative high pressure, all
anti-cyclones \footnote{henceforth abbreviated AC} effect an elevated \SSH \ie
a \textit{hill}. The algorithm ascends the full range of SSH levels where
contours were found. Consider an approximately Gaussian shaped AC that has a
peak SSH of say 5 increments larger than the average surrounding waters. 
As the algorithm approaches the sea surface from below, it will eventually run
into contours that are closed onto themselves and that encompass the AC. At
first these contours might be very large and encompass not only one but several
ACs and likely also cyclones, but as the algorithm continues upwards found
contour will get increasingly circular, describing some outer \textit{edge} of
the AC. Once the contour and its interior pass all of the tests the algorithm
will decide that an AC was found and write it and all its parameters to disk.
The AC's region \ie the interior of the contour will be flagged from here on.
Hence any inner contour further up the water column will not pass the tests.
Once all AC's are found for a given time-step, the SSH flags get reset and the
entire procedure is repeated only this time \textit{descending} the SSH-range to
find cyclones. The tests for cyclones and anti-cyclones are identical except for
a factor $-1$ where applicable. In the following the most important steps of the
analysis are
outlined.
% ----------------------------------------------------------------------
\subsubsection{NaN-Check Contour}
\mcode{function CR_RimNan}\\
The first and most efficient test is to check whether indices of the
contour are already flagged. Contours within an already found eddy get thereby
rejected immediately.
%\subsubsection{Closed Ring}
%\mcode{function CR_ClosedRing}\\
%Contours that do not close onto themselves are obviously not eligible for
%further testing.
% ----------------------------------------------------------------------
\subsubsection{Sub-Window}
\mcode{function get_window_limits}\\
\mcode{function EddyCut_init}\\
For further analysis a sub-domain around the eddy is cut out of the SSH data.
These functions determine the indices of that window and subtract the
resultant offset for the contour indices. 
% ----------------------------------------------------------------------
\subsubsection{Logical Mask of Eddy Interiour}\label{filter:cutmask}
\mcode{function EddyCut_mask}\\
Basically this function creates a
\href{http://en.wikipedia.org/wiki/Flood_fill}{flood-fill} logical mask of the
eddy-interior. This is by far the most calculation intensive part of the whole
filtering procedure. A lot more time was wasted on attempting to solve this
problem more efficiently than time could have been saved would said attempts
have
been successful. The current solution is basically just MATLAB's
\mcode{imfill.m}, which was also used in the very first version of 09/2013.
EDIT: \mcode{imfill.m} was replaced by using \mcode{inpoly.m} to determine which indices lie within the contour-polygon. This method seems to be more exact at determining whether the inside-part of one grid cell (with respect to the smooth, spline-interpolated contour) is larger than the outside part or not.  
% ----------------------------------------------------------------------
%\subsubsection{Islands}
%\mcode{function CR_Nan}\\
%No flags within the eddy are allowed. This check also avoids contours around
%islands as all land is flagged \textit{a priori}.
% ----------------------------------------------------------------------
\subsubsection{Sense}
\mcode{function CR_sense}\\
\textbf{All} of the interior SSH values must lie either above or below current
contour level, depending on whether anti-cyclones or cyclones are sought.
% ----------------------------------------------------------------------
\subsubsection{Area} \label{filter:area}
\mcode{function getArea}\\
The main goal here is to determine the most exact area encompassed by the exact coordinates of the contour. It does so via MATLAB's \mcode{polyarea} function. This area is not related to the scale $\sigma$ that is determined in \ref{filter:dynscale}. It is however the relevant scale for \ref{filter:shape}.
\mcode{function EddyCircumference}\\
Circumference \eg line-length described by the contour. This is the other parameter needed for \ref{filter:shape}. This is however neither related to the actual eddy scale determined in
\ref{filter:dynscale}.
% ----------------------------------------------------------------------
\subsubsection{Cirumference}
\mcode{function EddyCircumference}\\
Line-length (sum of Euclidean norm of all nodes) of the contour.
\newpage
% ----------------------------------------------------------------------
\subsubsection{Shape}\label{filter:shape}
\mcode{function CR_Shape}\\
\begin{wrapfigure}{r}{0.4\textwidth}
	\includegraphics[width=0.45\textwidth]{isoper}
	\caption{Different values of the isoperimetric quotient.}
	\label{fig:isoper}
\end{wrapfigure}
This is the crucial part of deciding whether the object is \textit{round
enough}. A perfect vortex with $\dpr{u}{y}=-\dpr{v}{x}$ is necessarily a
circle. The problem is that eddies get formed, die, merge, run into obstacles,
get asymmetrically advected etc. To successfully track them it is therefor
necessary to allow less circle-like shapes whilst still avoiding to \eg count 2
semi merged eddies as one. 
This is achieved by calculating the \hyperref[def:IQ]{isoperimetric quotient},
defined as the ratio of a ring's area to the area of a circle with equal
circumference. \cite{Chelton2011} use a similar method. They
require:\\ \textit{The distance between any pair of points within the connected
region must be less than a specified maximum} \citep{Chelton2011}.\\
While this method clearly avoids overly elongated shapes it allows for stronger
deformation within its distance bounds. \todo{compare both, show pictures}
% ----------------------------------------------------------------------
\subsubsection{Amplitude} \label{filter:amp}
\mcode{function CR_AmpPeak}\\
This function determines the amplitude \ie the maximum of the absolute
difference between SSH and current contour level and the position thereof as
well as the amplitude relative to the mean SSH value of the eddy interior as
done by \cite{Chelton2011}. The
amplitude is then tested against the user-given threshold. The function also
creates a matrix with current contour level shifted to zero and all values
outside of the eddy set to zero as well. 
% ----------------------------------------------------------------------
\subsubsection{Chelton's Scales} \label{filter:chstuff}
\mcode{function cheltStuff}\\
\cite{chelton2011} introduced 4 different eddy-scales.
\begin{enumerate}
	\item
	The effective scale $L_{eff}$ as the radius of a circle with its area equal to that enclosed by the contour.
	\item
	The scale $L_e  = L(z=\expp{-1} a)$ where $a$ as the amplitude with reference to the original contour and the $z$-axis zero-shifted to that contour. In other words the effective scale of the contour that is calculated at $1/\expp{}$ of the original amplitude.
	\item
	The scale $L=L_e/\sqrt{2}$.
	\item
	The scale $L_s$ which is \textit{a direct estimate based on the contour of SSH within the eddy interior around which the average geostrophic speed is maximum.} \citep{chelton2011} It is hence conceptually the same as $\sigma$ \todoil{legend ref}. This scale was not calculated here, as I could not think of an efficient simple way to estimate the area bounded by maximum geostrophic speed \ie the zero-vorticity contour. To understand why this would be difficult to achieve see also sections... \todoil{ref to approp sections eg noise in vort etc}.  
\end{enumerate}
% ----------------------------------------------------------------------
\subsubsection{Profiles}\label{filter:profiles}
\mcode{function EddyProfiles}\\
This step
\begin{itemize}
\item 
 saves the meridional and zonal profiles of SSH, U and V through  the eddie's peak and spanning the entire sub-domain as described in \ref{filter:cutmask}.
\item
creates spline functions from the ssh-profiles and uses them to interpolate the profiles onto 100-piece equi-distant coordinate vectors to buid smooth interpolated versions of ssh-profiles in both directions.
\item
in turn uses the splined data to create smooth 4-term Fourier-series functions for the profiles.
\end{itemize}
\todoil{show plots fourier/non fourier etc}
\begin{wrapfigure}{r}{0.4\textwidth}
	\includegraphics[width=0.4\textwidth]{profiles}
	\caption{Zonal $x$- and $z$-normalized cyclone-profiles (early data $\sim$ '13/12).}
	\label{fig:profiles}
\end{wrapfigure}
% ----------------------------------------------------------------------
\subsubsection{\textit{Dynamic} Scale ($\sigma$)} \label{filter:dynscale}
\mcode{function EddyRadiusFromUV}\\
The contour line that is being used to detect the eddy is not
necessarily a good measure of the eddy's \textit{scale} \ie it doesn't
necessarily represent the eddy's outline very well. This becomes very 
obvious when the area, as inferred by \ref{filter:area}, is plotted over time
for an already successfully tracked eddy. The result is not a smooth curve at
all. This is so because at different time steps the eddy usually gets detected
at different contour levels. Since its surrounding continuously changes and
since the eddy complies with the testing-criteria the better the closer the
algorithm gets to the eddy's peak value, the determined area of the contour
jumps considerably between time steps. This is especially so for large flat
eddies with amplitudes on the order of $1cm$. If the contour increment is on
that scale as well, the difference in contour-area between two time steps 
easily surpasses $100\%$ and more. 
Since there is no definition for the \textit{edge} of an eddy, it is here
defined as the ellipse resulting from the meridional and zonal diameters that
are the distances between first minimum and maximum orbital velocity, away from
the eddy's peak in positive and negative y and x directions respectively. 
In the case of a meandering jet with a maximum flow speed at its center, that
is shedding off an eddy, this scale corresponds to half the distance between
two opposing center-points of the meander. It is also the distance at which a
change in vorticity-polarity occurs and is thus assumed to be the most plausible
\textit{dividing line} between vortices. \\
Trying to determine the location where this sign change in vorticity occurs in the profiles turns out to be very tricky. What we seek are local extremata of the geostrophic speeds \ie of the ssh-gradients $h_x$. In a perfect Gaussian-shaped eddy, these would simply correspond to the first local extremata of $h_x$ away from the peak. In \textit{reality} the eddies can be very wobbly with numerous local maxima and minima in the gradients of their flanks. One could argue, that it must be the largest extremata, as it is the highest geostrophic speeds that are sought. In practice \footnote{especially for the high-resolution model data.} multiple superimposed signals of different scales often create very strong gradients locally. But the main issue here is that one weak eddy adjacent to one strong eddy also has the stronger gradients of the stronger one within its domain so that simply looking for the fastest flow speeds along the profiles is insufficient. It is also not possible to restrict the cut domain to the extent of a single eddy only, because at the point where the domain is selected, we do not know yet whether the detection algorithm \textit{took bait} at the eddy's base or later close to the tip. \\
The best method thus far seems to be to use the Fourier-series functions from \ref{filter:profiles} to determine the first extremata away from the eddy's peak. The Fourier order was chosen to be 4 by trial and error. The effect is that small-scale low-amplitude noise is avoided, allowing for more reliable determinations of $\grad^2 h_{fou} = 0$       \\
 Once the zero crossings in all 4 directions are found, their mean is taken as the eddy's scale. 
% ----------------------------------------------------------------------
\subsubsection{\textit{Dynamic} Amplitude} \label{filter:ampDyn}
\mcode{function EddyAmp2Ellipse}\\
As mentioned above, the contour that helps to detect the eddy is not
representative of its extent. This is also true for the $z$-direction, for the
same reasons. This function therefor takes an SSH-mean at indices of the ellipse
created by the determined zonal and meridional \textit{dynamical} diameters,
and uses this as the basal value to determine a \textit{dynamic} amplitude.
% ----------------------------------------------------------------------
\subsubsection{Center of Volume} \label{filter:CoV}
%\mcode{function EddyArea2Ellipse}\\
\mcode{function CenterOfVolume}\\
Instead of using the geo-position of the eddy's peak in the tracking procedure,
it was decided to instead use the center of the volume created by the basal
shifted matrix from \ref{filter:amp} \ie \textit{the center of volume of the
dome
(resp. valley) created by capping off the eddy at the contour level}.
This method was chosen because from looking at animations of the tracking
procedure
it became apparent that, still using peaks as reference points, the eddy
sometimes jumped considerably from one time step to the next if two local maxima
existed within the eddy. \Eg in one time-step local maximum $A$ might be just a
little bit larger than local maximum $B$ and one time-step later a slight shift
of mass pushes local maximum $B$ in pole position, creating a substantial jump
in the eddy-identifying geo-position hence complicating the tracking procedure. 
% ----------------------------------------------------------------------
\subsubsection{Geo Projection}\label{filter:projLocs}
%\mcode{function EddyArea2Ellipse}\\
\mcode{function ProjectedLocations}\\
An optional threshold on the distance an eddy is allowed to travel over one time-step is implemented in the tracking algorithm \ref{S:04}. This is a direct adaptation of the ellipse-based constraint described by \cite{chelton2011}. The maximum distance in western direction traveled by the eddy within one time-step is limited according to $x_{west} = \alpha c \delta{t} $ where $c$ as the local long-Rossby-wave phase-speed and \eg $\alpha=1.75$. In eastern direction the maximum is fixed at a value of \eg $x_{east} = 150\mathrm{km}$. This value is also used to put a lower bound on $x_{west}$ and for half the minor axis ($y$-direction) of the resultant ellipse.    
















% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Step S05: Track Eddies} \label{S:05}
\mcode{function function S04_track_eddies}\\
Due to the the relatively fine temporal resolution (daily) of the model data,
the tracking procedure turns out to be much simpler than the one described by
\cite{Chelton2007}. There is really no need to project the new position of an
eddy, as it generally does not travel further than its own scale in one day.
This means that one eddy can be unambigiously tracked from one time step to the
next as long both time-steps agree on which eddy from the other
time-step is located the least distance away.
The algorithm therefor simply builds an arc-length-distance matrix
between all old and all new eddies and then determines the minima of that
matrix in both directions \ie one array for the new with respect to the old,
and one for the old with respect to the new set. This leads to the following
possible situations:
\begin{itemize}
	\item 
	Old and new agree on a pair. \Ie old eddy $O_a$ has a closest neighbour in
the new set $N_a$ and $N_a$ agrees that $O_a$ is the closest eddy from the old
set. Hence the eddy is tracked.  $N_a$ is $O_a$ at a later time.
\item
$N_a$ claims $O_a$ to be the closest, but $N_b$ makes the same claim. \Ie two
eddies from the new set claim one eddy from the old set to be the closest. 
In this situation the closer one is decided to be the old one at a later
time-step and the other one must be a newly formed eddy.
\item 
At this point all new eddies are either allocated to their respective old
eddies or assumed to be \textit{newly born}. The only eddies that have not been
taken care of are those from the old set, that \textit{lost} ambigious claims to
another old eddy, that was closer to the same claimed new eddy. \Ie there is no
respective new eddy available which must mean that the eddy just \textit{died}.
In this case the entire track with all the information for each time step is
saved as long as the track-length meets the threshold criterium. If it doesn't,
the track is deleted. 
\end{itemize}

\section{Step S05: Cross Reference Old to New Indices}
\mcode{function function S05_init_output_maps}\\
The main purpose of this step is to allocate all grid nodes of the input data
to the correct node of the output map. Since the output map is usually much
coarser than the input data there is no need for interpolation.  

\section{Step S06: Make Maps of Mean Parameters}
\mcode{function function S06_analyze_tracks}\\
 

\section{Example}
To show a hands-on example an exemplary run for part of the North Atlantic is
demonstrated in the following. 
\subsection{Map Parameters} \label{codeEx:map_params}
The geo-coordinates for the map window are to be set in \mcode{map_vars.m}.
In this case a small region in the eddy-rich North Atlnatic from $-90^\circ$
west till $-40^\circ$ west and from $20^\circ$ north till $60^\circ$ north is
chosen. This script has an effect only on the very first step (\mcode{S_00..}).
\begin{lstlisting}
function MAP=map_vars
	%% user input
	MAP.geo.west=-90;
	MAP.geo.east=-40;
	MAP.geo.south=20;
	MAP.geo.north=60;
	MAP.time.delta_t = 1; % [days]
	MAP.SSH_unitFactor = 100; % eg 100 if SSH data in cm, 1/10 if in deka m etc..
	MAP.pattern.in='SSH_GLB_t.t0.1_42l_CORE.yyyymmdd.nc';
end
\end{lstlisting}
\todo{show map}
\subsection{Other Parameters}
All other parameters are to be set in \mcode{input_vars.m}. 
\begin{itemize}
	\item 
	The maximum number of \textit{local workers} (threads) is at default settings
usually limited to 12 by Matlab. Hence 
 \begin{lstlisting}
 	U.threads.num=12;
 \end{lstlisting}
 \item 
 The data available at this point in time spans from 1994/04/02 till 2006/12/31.
Hence
 \begin{lstlisting}
 	U.time.from.str='19940402';
  	U.time.till.str='20061231';
 \end{lstlisting}
\item 
Data is available per day. Therefor \mcode{U.time.delta_t=1; } is set
to 1. This could also be changed to another value, if the user wished to skip
time-steps.
	\item \label{codeEx:data}
	The path \mcode{U.path.root='../dataXMPL/';} can be set arbitrarily. If it
does not yet exist, it will be created. It is suggested to not reuse a path,
as this could lead to inconsitencies in the data.
\item
\todo{ \mcode{	U.path.TempSalt.name='TempSalt/';}}
\item
\mcode{U.path.raw.name='/scratch/uni/ifmto/u241194/DAILY/EULERIAN/SSH/';}\\
This is where the SSH data is stored.
\item
\begin{lstlisting}
	U.contour.step=0.01; % [SI]
	U.thresh.radius=5e3; % [SI]
	U.thresh.amp=0.02; % [SI]
	U.thresh.shape.iq=0.5; % isoperimetric quotient
	U.thresh.shape.chelt=0.5; % (diameter of circle with equal area)/(maximum
distance between nodes) (if ~switch.IQ) 
	U.thresh.corners=6; % min number of data points for the perimeter of an eddy
	U.thresh.dist=1*24*60^2; % max distance travelled per day
	U.thresh.life=20; % min num of living days for saving
\end{lstlisting}
In this segment the following values are set (all in SI-units, except for
time dimension).:
	\begin{itemize}
		\item 
		The contour intervall with which to look for SSH-contours from maximum to
	minimum SSH-value.
	\item
	The minimum radius threshold for an eddy.
	\item
	The minimum amplitude threshold.
	\item
	Either the minimum $\IQ$ value or the minimum ratio of the diameter of a
circle	 with equal area over the maximum distance between nodes, depending on
which	 method is chosen (see ). 
	\item
	The minimum number of grid nodes making up the contour.
	\item
	The maximum distance-travelled-per-timestep threshold.
	\item
	The minium track-length (in time) threshold for a track to be saved.
	\end{itemize}
\item

\begin{lstlisting}
	U.dim.X=40*1+1;
	U.dim.Y=40*1+1;
	U.dim.west=-90;
	U.dim.east=-50;
	U.dim.south=20;
	U.dim.north=60;
	U.dim.NumOfDecimals=1;
\end{lstlisting}
These paramters describe the output maps, that will be created in the very end
(\eg maps of mean values). Ideally they are set to the same limits as
those set in \ref{codeEx:map_params}. \mcode{U.dim.X}/\mcode{U.dim.Y} dictate
the size of the output maps. They can be set arbitrarilly, but the format \eg
$(east-west)*n +1; \;\; n \in \mathrm{N}$ is suggested to avoid long decimals in
the coordinate matrices.
\item
\begin{lstlisting}
	U.switchs.RossbyStuff=false;
	U.switchs.IQ=true;	
\end{lstlisting}
Choose whether step \mcode{S01b_BruntVaisRossby} is to be run (needs adequate
salt and temperature files \todo{explain}) and the type of shape-testing (
\mcode{U.switchs.IQ=true}) for $\IQ$-method.
\item
The rest are less important technical things that are only relevant if the code
itself is modulated (\eg if modules are added).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Running the Code}
The seperate steps can be run all at once via \mcode{Sall.m} or one by one, as
long as they are started consecutively in the order indicated by their name
( \mcode{S00..}, then \mcode{S01..} \etc). \mcode{S01b} is not necessary
though. Each step saves its own files which are then read by the next step.
All output data is saved in the user given root-path from \ref{codeEx:data}.
This concept uses quite a lot of disk space and is also quite substantially
slowed by all the reading and writing procedures. The benefits, on the other
hand, are that debugging becomes much easier. If the code fails at some step,
at least all the calculations up to that step are dealt with and do not need to
be re-run. The concept also makes it easy to extend the code by further
add-ons.

\todo{plots to follow...}



\href{https://github.com/PremKolar/MT4}{see github for progress}
















